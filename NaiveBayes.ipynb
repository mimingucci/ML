{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM45BsJXT/Igb0vRJEMxqi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimingucci/ML/blob/main/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes assumes the random variables within $x_n$ are independent conditional on the class of observation n. I.e. if $x_n \\in \\mathbb{R}^D\n",
        "$ , Naive Bayes assumes<br/>\n",
        "$p(x_n|Y_n) = p(x_{n1}|Y_n)\\cdot p(x_{n2}|Y_n) \\cdot ... \\cdot p(x_{nD}|Y_n).$\n",
        "<br/>\n",
        "**Making Classifications**\n",
        "<br/>\n",
        "Consider a test observation $x_0$. For k=1,…,K\n",
        ", we use Bayes’ rule to calculate<br/>\n",
        "$\\begin{split}\n",
        "\\begin{align*}\n",
        "p(Y_0 = k|x_0) &\\propto p(x_0|Y_0 = k)p(Y_0 = k)\n",
        "\\\\\n",
        "&= \\hat{p}(x_0|Y_0 = k)\\hat{\\pi}_k,\n",
        "\\end{align*}\n",
        "\\end{split}\n",
        "$  <br/>\n",
        "where $\\hat{p}$ gives the estimated density of $x_0$ conditional on $Y_0$. We then predict $Y_0$=k\n",
        " for whichever value k\n",
        " maximizes the above expression."
      ],
      "metadata": {
        "id": "xKb1sYqLbbzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m6w52ZR6FDC"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "    #Fit Model\n",
        "\n",
        "    def _estimate_class_parameters(self, X_k):\n",
        "\n",
        "        class_parameters = []\n",
        "\n",
        "        for d in range(self.D):\n",
        "            X_kd = X_k[:,d] # only the dth column and the kth class\n",
        "\n",
        "            if self.distributions[d] == 'normal':\n",
        "                mu = np.mean(X_kd)\n",
        "                sigma2 = np.var(X_kd)\n",
        "                class_parameters.append([mu, sigma2])\n",
        "\n",
        "            if self.distributions[d] == 'bernoulli':\n",
        "                p = np.mean(X_kd)\n",
        "                class_parameters.append(p)\n",
        "\n",
        "            if self.distributions[d] == 'poisson':\n",
        "                lam = np.mean(X_kd)\n",
        "                class_parameters.append(p)\n",
        "\n",
        "        return class_parameters\n",
        "\n",
        "    def fit(self, X, y, distributions = None):\n",
        "\n",
        "        #Record info\n",
        "        self.N, self.D = X.shape\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        if distributions is None:\n",
        "            distributions = ['normal' for i in range(len(y))]\n",
        "        self.distributions = distributions\n",
        "\n",
        "\n",
        "        #Get prior probabilities\n",
        "        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n",
        "        self.pi_ks = unique_y_counts/self.N\n",
        "\n",
        "\n",
        "        #Estimate parameters\n",
        "        self.parameters = []\n",
        "        for i, k in enumerate(self.unique_y):\n",
        "            X_k = self.X[self.y == k]\n",
        "            self.parameters.append(self._estimate_class_parameters(X_k))\n",
        "\n",
        "\n",
        "    #Make Classifications\n",
        "\n",
        "    def _get_class_probability(self, x_n, j):\n",
        "\n",
        "        class_parameters = self.parameters[j] # j is index of kth class\n",
        "        class_probability = 1\n",
        "\n",
        "        for d in range(self.D):\n",
        "            x_nd = x_n[d] # just the dth variable in observation x_n\n",
        "\n",
        "            if self.distributions[d] == 'normal':\n",
        "                mu, sigma2 = class_parameters[d]\n",
        "                class_probability *= sigma2**(-1/2)*np.exp(-(x_nd - mu)**2/sigma2)\n",
        "\n",
        "            if self.distributions[d] == 'bernoulli':\n",
        "                p = class_parameters[d]\n",
        "                class_probability *= (p**x_nd)*(1-p)**(1-x_nd)\n",
        "\n",
        "            if self.distributions[d] == 'poisson':\n",
        "                lam = class_parameters[d]\n",
        "                class_probability *= np.exp(-lam)*lam**x_nd\n",
        "\n",
        "        return class_probability\n",
        "\n",
        "    def classify(self, X_test):\n",
        "\n",
        "        y_n = np.empty(len(X_test))\n",
        "        for i, x_n in enumerate(X_test): # loop through test observations\n",
        "\n",
        "            x_n = x_n.reshape(-1, 1)\n",
        "            p_ks = np.empty(len(self.unique_y))\n",
        "\n",
        "            for j, k in enumerate(self.unique_y): # loop through classes\n",
        "\n",
        "                p_x_given_y = self._get_class_probability(x_n, j)\n",
        "                p_y_given_x = self.pi_ks[j]*p_x_given_y # bayes' rule\n",
        "\n",
        "                p_ks[j] = p_y_given_x\n",
        "\n",
        "            y_n[i] = self.unique_y[np.argmax(p_ks)]\n",
        "\n",
        "        return y_n\n"
      ]
    }
  ]
}