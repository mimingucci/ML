{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCg+MlNHWJp+KrYfgt+/6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimingucci/ML/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Init Network"
      ],
      "metadata": {
        "id": "W6P6mhXPIxrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume a sample neurol network with 2 layers include 1 hidden layer and 1 output layer"
      ],
      "metadata": {
        "id": "HeUFeTkHI9KL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL_WA_sQIZhe",
        "outputId": "603e680d-665d-4ec6-9430-b1fae1263fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
          ]
        }
      ],
      "source": [
        "from random import seed\n",
        "from random import random\n",
        "def initialize_network(n_inputs, n_hiddens, n_outputs):\n",
        "  network=list()\n",
        "  hidden_layer=[{'weights':[random() for i in range(n_inputs+1)]} for j in range(n_hiddens)]\n",
        "  network.append(hidden_layer)\n",
        "  output_layer=[{'weights':[random() for i in range(n_hiddens+1)]} for j in range(n_outputs)]\n",
        "  network.append(output_layer)\n",
        "  return network\n",
        "seed(1)\n",
        "network=initialize_network(2, 1, 2)\n",
        "for layer in network:\n",
        "  print(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Activation\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Formula: activation=sum(weight*input)+bias"
      ],
      "metadata": {
        "id": "MX-vmJmXKykD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activate(weights, inputs):\n",
        "  activation=weights[-1]#bias\n",
        "  for i in range(len(weights)-1):\n",
        "    activation+=weights[i]*inputs[i]\n",
        "  return activation"
      ],
      "metadata": {
        "id": "X3VdIq6rKyJo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer neurol using sigmoid, tanh, ... function"
      ],
      "metadata": {
        "id": "Urzvg71ZLsog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using sigmoid function (z=1/(1+e^(-z)))\n",
        "import math\n",
        "def transfer(activation):\n",
        "  return 1.0/(1.0+math.exp(-activation))"
      ],
      "metadata": {
        "id": "67A8ybtALrAi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Forward Propagation"
      ],
      "metadata": {
        "id": "mSIVzdHPMmZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagate(network, inputs):\n",
        "  for layer in network:\n",
        "    cur_network=list()\n",
        "    for neurol in layer:\n",
        "      activation=activate(neurol['weights'], inputs)\n",
        "      neurol['output']=transfer(activation)\n",
        "      cur_network.append(neurol['output'])\n",
        "    inputs=cur_network\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "3xcE1ncdMqr3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test forward_propagate\n",
        "print(network)\n",
        "inputs=[1, 2]\n",
        "outputs=forward_propagate(network, inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa-nC907OFIa",
        "outputId": "b3fd0759-b038-4141-a2c9-bfcc195bb5e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}], [{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
            "[0.6754093036624, 0.7445596419029368]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Backpropagation\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "Call sigmoid function is f, we have:\n",
        "```\n",
        "```\n",
        "Derivative of sigmoid function: (f)'=f*(1-f)"
      ],
      "metadata": {
        "id": "MKjTpuTNOoov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(f):\n",
        "  return f*(1.0-f)"
      ],
      "metadata": {
        "id": "xZ7jP4imOjqj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Backpropagation=(actual_output - expected)*derivative(actual_output) for output neurol\n",
        "\n",
        "---\n",
        "\n",
        "for hidden neurons error=(weight[k]* error[j])*deravative(output). Here, weight[k] represents weight of k'th neurol of following layer to current neuron, error[j] is the signal error from the j'th neuron in the output layer and output is output of current neuron."
      ],
      "metadata": {
        "id": "WoqLvP9HSq6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagate error and store in neurons\n",
        "# We store error value in 'delta'\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * derivative(neuron['output'])"
      ],
      "metadata": {
        "id": "gtX9Zk3tSomx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train Network\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "Here, this step is enable broke down into 2 sections:\n",
        "\n",
        "1. Update Weights according to formula: weight=weight-learning_rate * error * input\n",
        "2. Train using gradient descent\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O6g4pa9OXB9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, lr):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]# because last element is label\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] -= lr * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] -= lr * neuron['delta'] # update bias"
      ],
      "metadata": {
        "id": "VbfGPNWsXFah"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]) # squares errors\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) # trace"
      ],
      "metadata": {
        "id": "EKYzVMxyaJ4E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Predict"
      ],
      "metadata": {
        "id": "tTjJop2Aa1iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(network, row):\n",
        "  outputs=forward_propagate(network, row)\n",
        "  return outputs.index(max(outputs))"
      ],
      "metadata": {
        "id": "H_Sqi1-ca3cE"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}